{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from Labels import label\n",
    "import enchant\n",
    "import nltk\n",
    "from guess_language import guess_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class preprocessing(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #labels object\n",
    "        self.lbl= label()\n",
    "        self.d = enchant.Dict(\"en_US\") \n",
    "        pass\n",
    "    \n",
    "    def read_data(self):\n",
    "        \"\"\"\n",
    "        * Read all data files\n",
    "        \"\"\"\n",
    "        fb = pd.read_json('data-sample/facebook-rotterdam-20170131.json',lines=True)\n",
    "        go = pd.read_json('data-sample/google-rotterdam-20170207.json',lines=True)\n",
    "        fac = pd.read_csv('data-sample/factual-rotterdam-20170207.csv')\n",
    "        \n",
    "        return fb,go,fac\n",
    "    \n",
    "    def map_fb_loc(self,data,columName,dictName,drop_inst):\n",
    "        \"\"\"\n",
    "        * Map location of facebook into latitude and longitude values\n",
    "        \"\"\"\n",
    "        \n",
    "        #get location \n",
    "        if drop_inst:\n",
    "            data = data.drop(data.index[drop_inst])\n",
    "            \n",
    "        data['longitude'] = [i[dictName][1] for i in data[columName]]\n",
    "        data['latitude'] = [i[dictName][0] for i in data[columName]]\n",
    "\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def dropna(self,data,column_set,how):\n",
    "        \"\"\"\n",
    "        * Drop NAN values from a specific column\n",
    "        \"\"\"\n",
    "        \n",
    "        data = data.dropna(subset=column_set,how=how)\n",
    "        data = data.reset_index()\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def map_dict(self,data,columnName,dictName):\n",
    "        \"\"\"\n",
    "        * Extract useful values from dictionary\n",
    "        \"\"\"\n",
    "        \n",
    "        col_set = [data[columnName][i][dictName] for i in range(len(data))]\n",
    "        \n",
    "        return col_set\n",
    "    \n",
    "    def label_price(self,data):\n",
    "        \"\"\"\n",
    "        * Label price range/level into its corresponding category\n",
    "        \"\"\"\n",
    "        \n",
    "        data['price'] = np.NaN\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            if isinstance(data['price_range'][i], str):\n",
    "                data['price'][i] = data['price_range'][i].count('$')\n",
    "            elif not np.isnan(data['price_level'][i]):\n",
    "                data['price'][i] = data['price_level'][i]\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    def label_category(self,data,columnName):\n",
    "        \"\"\"\n",
    "        * Return a set of labeled data\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        labeled_data = [self.map_label_category(i) for i in data[columnName]]\n",
    "        \n",
    "        return labeled_data\n",
    "    \n",
    "    def map_label_category(self,category):\n",
    "        \n",
    "        \"\"\"\n",
    "        * Return label of category based on lexicon\n",
    "        \"\"\"\n",
    "        \n",
    "        tmp_wrd = nltk.wordpunct_tokenize(category)\n",
    "        cat_label = np.NaN\n",
    "    \n",
    "        for word in tmp_wrd:\n",
    "            if self.d.check(word):\n",
    "                unique = 0\n",
    "            \n",
    "                for i in range(len(self.lbl.categ)):\n",
    "                    if word.lower() in self.lbl.categ[i]:\n",
    "                        if unique != 1:\n",
    "                            cat_label = i\n",
    "                            unique = 1\n",
    "    \n",
    "        return cat_label  \n",
    "    \n",
    "    \n",
    "    def remove_stop_wds(self,data,columnName):\n",
    "        \"\"\"\n",
    "        * Return key words from columName\n",
    "        \"\"\"\n",
    "    \n",
    "        reviews = []\n",
    "\n",
    "        for review in data[columnName]:\n",
    "            \n",
    "            review = nltk.wordpunct_tokenize(review)\n",
    "            text_prosc = [word.lower() for word in review if not word.lower() in self.lbl.stops and word.isalpha()]\n",
    "            cleaned_text = \" \".join(text_prosc)\n",
    "            reviews.append(cleaned_text)\n",
    "            \n",
    "        return reviews\n",
    "    \n",
    "    def guess_language(self,data,columnName):\n",
    "        \"\"\"\n",
    "        * Return predicted language of a given columnName\n",
    "        \"\"\"\n",
    "        \n",
    "        data['language'] = [guess_language(text) for text in data[columnName]]\n",
    "        \n",
    "        return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
